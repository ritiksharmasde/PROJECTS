{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMf/uy4V6JxTcyHv78R+qOI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritiksharmasde/PROJECTS/blob/main/tokenization_in_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK0V0TBFw8um"
      },
      "outputs": [],
      "source": [
        "# Task 1: Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install spaCy model in Colab ---\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# --- Import libraries ---\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import spacy\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# --- Text input ---\n",
        "text = \"\"\"Artificial Intelligence is transforming the world.\n",
        "NLP is a key part of AI.\n",
        "Machines are learning to understand human language.\"\"\"\n",
        "\n",
        "# --- NLTK Sentence Tokenization ---\n",
        "print(\"NLTK Sentence Tokenization:\")\n",
        "print(sent_tokenize(text))\n",
        "\n",
        "# --- spaCy Sentence Tokenization ---\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "print(\"\\nspaCy Sentence Tokenization:\")\n",
        "print([sent.text for sent in doc.sents])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyRhvUP3xSgw",
        "outputId": "99cd431e-26a4-4604-9214-275c68069d43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Sentence Tokenization:\n",
            "['Artificial Intelligence is transforming the world.', 'NLP is a key part of AI.', 'Machines are learning to understand human language.']\n",
            "\n",
            "spaCy Sentence Tokenization:\n",
            "['Artificial Intelligence is transforming the world. \\n', 'NLP is a key part of AI. \\n', 'Machines are learning to understand human language.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Word Tokenization"
      ],
      "metadata": {
        "id": "h0a9V9JmyGK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "text = \"\"\"Artificial Intelligence is transforming the world.\n",
        "NLP is a key part of AI.\n",
        "Machines are learning to understand human language.\"\"\"\n",
        "\n",
        "print(\"NLTK Word Tokens:\")\n",
        "print(word_tokenize(text))\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"\\nspaCy Word Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nUnique tokens (spaCy):\")\n",
        "print(set(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFjgcj7zyJ22",
        "outputId": "35daa954-bcaf-448b-8826-75951adf9ba1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Word Tokens:\n",
            "['Artificial', 'Intelligence', 'is', 'transforming', 'the', 'world', '.', 'NLP', 'is', 'a', 'key', 'part', 'of', 'AI', '.', 'Machines', 'are', 'learning', 'to', 'understand', 'human', 'language', '.']\n",
            "\n",
            "spaCy Word Tokens:\n",
            "['Artificial', 'Intelligence', 'is', 'transforming', 'the', 'world', '.', '\\n', 'NLP', 'is', 'a', 'key', 'part', 'of', 'AI', '.', '\\n', 'Machines', 'are', 'learning', 'to', 'understand', 'human', 'language', '.']\n",
            "\n",
            "Unique tokens (spaCy):\n",
            "{'is', 'language', 'of', 'key', '\\n', 'a', 'Machines', '.', 'NLP', 'are', 'to', 'Artificial', 'the', 'Intelligence', 'AI', 'part', 'understand', 'world', 'transforming', 'learning', 'human'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part B: Stemming\n",
        "# Task 3: Apply Stemming\n"
      ],
      "metadata": {
        "id": "pGKqDw67yVft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"playing\", \"played\", \"plays\", \"happily\", \"happiness\", \"better\"]\n",
        "\n",
        "stems = [stemmer.stem(w) for w in words]\n",
        "\n",
        "print(\"Stemming Results:\", stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ-mS2fsyg2t",
        "outputId": "b5225448-c5aa-49ac-87b5-4c15800e63ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming Results: ['play', 'play', 'play', 'happili', 'happi', 'better']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part C: Lemmatization\n",
        "# Task 4: Lemmatization with NLTK"
      ],
      "metadata": {
        "id": "shPr6e9KywF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"running →\", lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
        "print(\"played →\", lemmatizer.lemmatize(\"played\", pos=\"v\"))\n",
        "print(\"happily →\", lemmatizer.lemmatize(\"happily\", pos=\"r\"))\n",
        "print(\"happiness →\", lemmatizer.lemmatize(\"happiness\", pos=\"n\"))\n",
        "print(\"better →\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP0_hKEWyzbV",
        "outputId": "af852d71-fb0f-4af0-e2a7-a5c42d1e2a56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running → run\n",
            "played → play\n",
            "happily → happily\n",
            "happiness → happiness\n",
            "better → good\n"
          ]
        }
      ]
    }
  ]
}